"""Script to incrementally load Iceberg Silver OHLCV into ClickHouse

Pattern:
- Read latest watermark from ClickHouse ETL state
- Read Iceberg Silver rows with _change_tstamp >= (watermark - lookback)
- Append into ClickHouse (dedupe handled by ReplacingMergeTree version column)
- Update watermark in ClickHouse ETL state after success
"""

import logging
import sys
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple, cast

import daft
import pyarrow as pa
import pyarrow.compute as pc
from daft import col, lit

from app.etl.utils.daft import iter_batches_by_ts
from common.consts import LOG_FORMAT
from data.clickhouse.base_executor import ClickHouseBaseExecutor
from data.clickhouse.base_inserter import ClickHouseBaseInserter
from models.consts import CHG_TS_COL
from models.iceberg.ohlcv.slv.binance import BinanceOHLCVSlv


# TODO: I don't think clickhouse-driver does NOT support timezone-aware datetimes

# SQL DDL files
ETL_STATE_MODEL_PATH = "models/clickhouse/etl/etl_state_current.sql"
BINANCE_OHLCV_SLV_MODEL_PATH = "models/clickhouse/ohlcv/slv/binance.ohlcv_slv.sql"

# ClickHouse tables
# We exclude _insert_tstamp from the target fields because it's auto-generated by ClickHouse
CH_ETL_STATE_TABLE = "etl.etl_state_current"
CH_TARGET_TABLE = "binance.ohlcv_slv"
CH_TARGET_FIELDS = [
    "exchange",
    "symbol",
    "event_tstamp",
    "open",
    "high",
    "low",
    "close",
    "volume",
    CHG_TS_COL,
]

# Job information
JOB_NAME = "binance_ohlcv_slv_to_clickhouse"
SOURCE = "iceberg"
SOURCE_IDENTIFIER = "binance.ohlcv_slv"
DEST_IDENTIFIER = CH_TARGET_TABLE

# Incremental settings
LOOKBACK = timedelta(minutes=15)
TS_BATCH_STEP = timedelta(minutes=30)
DF_BATCHSIZE = 200000


def _arrow_table_to_rows(tbl: pa.Table) -> List[Dict[str, Any]]:
    """Convert a PyArrow Table to list-of-dicts, normalizing datetimes for ClickHouse
    """
    # Select only the columns we expect to mirror into ClickHouse
    tbl = tbl.select([f for f in CH_TARGET_FIELDS if f in tbl.column_names])
    return tbl.to_pylist()


def _get_last_change_tstamp(ch: ClickHouseBaseExecutor, job_name: str) -> Optional[datetime]:
    """Return last watermark stored in ClickHouse
    """
    rows_any = ch.execute(
        f"""
        SELECT last__change_tstamp
        FROM {CH_ETL_STATE_TABLE}
        WHERE job_name = %(job_name)s
        ORDER BY _insert_tstamp DESC
        LIMIT 1
        """,
        params={"job_name": job_name},
    )
    if not rows_any:
        return None
    if not rows_any[0]:
        return None
    ts = rows_any[0][0]
    return ts if isinstance(ts, datetime) else None


if __name__ == "__main__":
    logging.basicConfig(format=LOG_FORMAT, level=logging.INFO)

    with ClickHouseBaseExecutor() as ch_executor:
        with open(BINANCE_OHLCV_SLV_MODEL_PATH, "r") as infile:
            ch_executor.execute(infile.read())
        with open(ETL_STATE_MODEL_PATH, "r") as infile:
            ch_executor.execute(infile.read())

        last_ts = _get_last_change_tstamp(ch_executor, JOB_NAME)

    if last_ts is None:
        logging.info("No existing ETL state found; will backfill from all available Silver data.")
        read_src_from = None
    else:
        read_src_from = last_ts - LOOKBACK
        logging.info(f"Last watermark: {last_ts}; reading from (watermark - lookback): {read_src_from}")

    slv_model = BinanceOHLCVSlv()
    slv_model.load_table()
    slv_df = daft.read_iceberg(slv_model.tbl_object)

    # Incremental filter
    if read_src_from is not None:
        logging.info(f"Filtering from {read_src_from}, where timezone is {read_src_from.tzinfo}")
        slv_df = slv_df.filter(col(CHG_TS_COL) >= lit(read_src_from))

    # Process in time-sliced batches based on _change_tstamp
    # As we must find the maximum seen _change_tstamp, we need to use to_arrow_iter to batch the DataFrame
    max__change_tstamp: Optional[datetime] = None
    with ClickHouseBaseInserter() as ch_inserter:
        for ts_batch in iter_batches_by_ts(input_df=slv_df, ts_col=CHG_TS_COL, step=TS_BATCH_STEP):
            batch_df = ts_batch.df
            batch_df = batch_df.into_batches(DF_BATCHSIZE)
            for rb in batch_df.to_arrow_iter():
                pa_tbl = pa.Table.from_batches([rb])

                if pa_tbl.num_rows == 0:
                    continue

                # Track max watermark in this chunk
                chunk_max = (pc.max(pa_tbl[CHG_TS_COL]).as_py() if CHG_TS_COL in pa_tbl.column_names else None)
                if isinstance(chunk_max, datetime):
                    # chunk_max = _to_ch_naive_utc(chunk_max)
                    max__change_tstamp = (
                        chunk_max if max__change_tstamp is None or chunk_max > max__change_tstamp 
                        else max__change_tstamp
                    )

                ch_inserter.insert(
                    tbl_name=CH_TARGET_TABLE,
                    data=_arrow_table_to_rows(pa_tbl),
                    field_names=CH_TARGET_FIELDS,
                )

    if max__change_tstamp is None:
        logging.info("No rows found to load into ClickHouse; exiting.")
        sys.exit(0)

    # Write ETL state
    # TODO: If this part fails, we need a mechanism to ensure correct ETL data is processed
    state_row = {
        "job_name": JOB_NAME,
        "source": SOURCE,
        "source_identifier": SOURCE_IDENTIFIER,
        "dest_identifier": DEST_IDENTIFIER,
        "last__change_tstamp": max__change_tstamp,
        "_insert_tstamp": datetime.now(timezone.utc),
    }
    with ClickHouseBaseInserter() as ch_inserter:
        ch_inserter.insert(
            tbl_name=CH_ETL_STATE_TABLE,
            data=[state_row],
            field_names=list(state_row.keys()),
        )

    logging.info("Updated ETL state watermark to %s", max__change_tstamp)
