"""Script to incrementally load Iceberg Silver OHLCV into ClickHouse

Pattern:
- Read latest watermark from ClickHouse ETL state
- Read Iceberg Silver rows with _change_tstamp >= (watermark - lookback)
- Append into ClickHouse (dedupe handled by ReplacingMergeTree version column)
- Update watermark in ClickHouse ETL state after success
"""

import logging
import sys
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple, cast

import daft
import pyarrow as pa
import pyarrow.compute as pc
from daft import col, lit

from common.configs import Config, OsVariable
from common.consts import LOG_FORMAT
from data.clickhouse.base_executor import ClickHouseBaseExecutor
from data.clickhouse.base_inserter import ClickHouseBaseInserter
from models.iceberg.ohlcv.slv.binance import BinanceOHLCVSlv

# TODO: I don't think clickhouse-driver does NOT support timezone-aware datetimes

# SQL DDL files
ETL_STATE_MODEL_PATH = "models/clickhouse/etl/etl_state_current.sql"
BINANCE_OHLCV_SLV_MODEL_PATH = "models/clickhouse/ohlcv/slv/binance.ohlcv_slv.sql"

# ClickHouse tables
# We exclude _insert_tstamp from the target fields because it's auto-generated by ClickHouse
CH_ETL_STATE_TABLE = "etl.etl_state_current"
CH_TARGET_TABLE = "binance.ohlcv_slv"
CH_TARGET_FIELDS = [
    "exchange",
    "symbol",
    "event_tstamp",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "src_change_tstamp",
]

# Job information
JOB_NAME = "binance_ohlcv_slv_to_clickhouse"
SOURCE = "iceberg"
SOURCE_IDENTIFIER = "binance.ohlcv_slv"
DEST_IDENTIFIER = CH_TARGET_TABLE

# Incremental settings
LOOKBACK = timedelta(minutes=15)
TS_BATCH_STEP = timedelta(minutes=30)
DF_BATCHSIZE = 1_000_000


def _get_last_src_change_tstamp(ch_executor: ClickHouseBaseExecutor, job_name: str) -> Optional[datetime]:
    """Return last watermark stored in ClickHouse
    """
    rows_any = ch_executor.execute(
        f"""
            SELECT last_src_change_tstamp
            FROM {CH_ETL_STATE_TABLE}
            WHERE job_name = %(job_name)s
            ORDER BY _change_tstamp DESC
            LIMIT 1
        """,
        params={"job_name": job_name},
    )
    if not rows_any:
        return None
    if not rows_any[0]:
        return None
    if not rows_any[0][0]:
        return None
    ts = rows_any[0][0][0]
    return ts if isinstance(ts, datetime) else None


if __name__ == "__main__":
    logging.basicConfig(format=LOG_FORMAT, level=logging.INFO)

    with ClickHouseBaseExecutor() as ch_executor:
        with open(BINANCE_OHLCV_SLV_MODEL_PATH, "r") as infile:
            ch_executor.execute(infile.read())
        with open(ETL_STATE_MODEL_PATH, "r") as infile:
            ch_executor.execute(infile.read())

        last_ts = _get_last_src_change_tstamp(ch_executor, JOB_NAME)

    read_src_from = None
    if last_ts is None:
        logging.info("No existing ETL state found; will backfill from all available Silver data.")
    else:
        read_src_from = last_ts - LOOKBACK
        logging.info(f"Last source watermark: {last_ts}; reading from (watermark - lookback): {read_src_from}")

    slv_model = BinanceOHLCVSlv()
    slv_model.load_table()
    slv_df = daft.read_iceberg(slv_model.tbl_object)
    delta_df = slv_df

    # Incremental filter: we only filter the delta DF if there is a last watermark
    if read_src_from is not None:
        logging.info(f"Filtering from {read_src_from}, where timezone is {read_src_from.tzinfo}")
        delta_df = delta_df.filter(col("src_change_tstamp") >= lit(read_src_from))

    # Since we have MergeTree on ClickHouse, we may not need to iter by timestamp here
    # Instead, just iter by partition to control resource usage
    max_src_change_tstamp: Optional[datetime] = None
    delta_df = delta_df.into_batches(DF_BATCHSIZE)

    # Find the max src_change_tstamp in the delta DataFrame
    # Using Daft agg is simpler but not sure about memory efficiency
    # src_chg_tstamp_df = delta_df.agg(col("src_change_tstamp").max()).collect()
    # max_src_change_tstamp = src_chg_tstamp_df.to_pydict()["src_change_tstamp"][0]
    for part in delta_df.select("src_change_tstamp").iter_partitions():
        pa_tbl = part.to_arrow()

        if pa_tbl.num_rows == 0:
            continue

        chunk_max = pc.max(pa_tbl["src_change_tstamp"]).as_py()
        if isinstance(chunk_max, datetime):
            if max_src_change_tstamp is None or chunk_max > max_src_change_tstamp:
                max_src_change_tstamp = chunk_max

    if not max_src_change_tstamp:
        logging.info("No rows found to load into ClickHouse; exiting.")
        sys.exit(0)

    delta_df.write_clickhouse(
        table=CH_TARGET_TABLE,
        host=Config.os_get(key=OsVariable.CLICKHOUSE_HOST.value),
        port=int(Config.os_get(key=OsVariable.CLICKHOUSE_HTTP_PORT.value)),
        user=Config.os_get(key=OsVariable.APP_INSERTER_USER.value),
        password=Config.os_get(key=OsVariable.APP_INSERTER_PASSWORD.value),
        database="binance",
    )

    # Write ETL state
    # TODO: If this part fails, we need a mechanism to ensure correct ETL data is processed
    state_row = {
        "job_name": JOB_NAME,
        "source": SOURCE,
        "source_identifier": SOURCE_IDENTIFIER,
        "dest_identifier": DEST_IDENTIFIER,
        "last_src_change_tstamp": max_src_change_tstamp,
        # CHG_TS_COL: datetime.now(timezone.utc),  # Comment out for now to see if it merges using an auto column
    }
    with ClickHouseBaseInserter() as ch_inserter:
        ch_inserter.insert(
            tbl_name=CH_ETL_STATE_TABLE,
            data=[state_row],
            field_names=list(state_row.keys()),
        )

    logging.info("Updated ETL state watermark to %s", max_src_change_tstamp)
